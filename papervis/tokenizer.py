#!/usr/bin/env python3

import string

import nltk

from nltk.tokenize.punkt import PunktSentenceTokenizer
from nltk.tokenize import sent_tokenize, word_tokenize

# from sklearn.feature_extraction.text import TfidfVectorizer
#
# from plchkx.similarity import StemmerSimilarity
# from plchkx.search import Search
#
# import random
# import time

class Tokenizer:
    def __init__(self):
        pass
